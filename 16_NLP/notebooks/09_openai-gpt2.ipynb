{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Lenguaje de OpenAI\n",
    "\n",
    "A mitad de febrero, [OpenAI publicó un modelo de lenguaje](https://blog.openai.com/better-language-models/) capaz de generar lenguaje natural de formar coherente. Este modelo es generalista y, a pesar de ello, es capaz de rivalizar con los mejores sistemas específicos en tareas como comprensión automática de lenguaje natural, traducción automática, búsqueda de respuestas y resumen automático.\n",
    "\n",
    "Este modelo, llamado GPT-2, es el resultado de haber entrenado con 8 millones de páginas web (40 GB) con 1 500 millones de parámetros con un único objetivo: predecir cuál es la siguiente palabra.\n",
    "\n",
    "Sin embargo, OpenAI no ha publicado el modelo para evitar que alguien con malas intenciones pueda hacer un uso dañino de esta tecnología. Sí que han publicado una versión simplificada y más pequeña, y el paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), en el que explican todo el proceso.\n",
    "\n",
    "Con ganas y GPUs suficientes (+ tiempo y dinero), se puede replicar el proceso. Otras lecturas interesantes, sobre el tema: \n",
    "\n",
    "- [OpenAI's new Multitalented AI Writes, Translates, and Slanders](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2)\n",
    "- [Some thoughts on zero-day threats in AI, and OpenAI's GP2](https://www.fast.ai/2019/02/15/openai-gp2/)\n",
    "\n",
    "\n",
    "Este código de ejemplo está inspirado en [un tweet de Thomas Wolf](https://twitter.com/Thom_Wolf/status/1097465312579072000), de [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting torchvision_nightly\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/d8/6bb4584b20459cc057f1b922fe7e4757d005c812d2cf73d2903a86a8afd6/torchvision_nightly-0.2.3-py2.py3-none-any.whl\n",
      "Collecting six (from torchvision_nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting pillow>=4.1.1 (from torchvision_nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/c2/f84b1e57416755e967236468dcfb0fad7fd911f707185efc4ba8834a1a94/Pillow-6.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tqdm (from torchvision_nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, six, pillow, tqdm, torchvision-nightly\n",
      "Successfully installed numpy-1.16.2 pillow-6.0.0 six-1.12.0 torchvision-nightly-0.2.3 tqdm-4.31.1\n",
      "Collecting torch_nightly\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch_nightly-1.1.0.dev20190413-cp36-cp36m-linux_x86_64.whl (100.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 100.6MB 11kB/s \n",
      "\u001b[?25hInstalling collected packages: torch-nightly\n",
      "Successfully installed torch-nightly-1.1.0.dev20190413\n",
      "Collecting pytorch_pretrained_bert\n",
      "  Using cached https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl\n",
      "Collecting numpy (from pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting boto3 (from pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/ef/f81d8c35c7254fe2af9a1a8e034f07b88a824a441f37955d0a07a90b8ec7/boto3-1.9.130-py2.py3-none-any.whl\n",
      "Collecting requests (from pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl\n",
      "Collecting torch>=0.4.1 (from pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/31/ca/dd2c64f8ab5e7985c4af6e62da933849293906edcdb70dac679c93477733/torch-1.0.1.post2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tqdm (from pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl\n",
      "Collecting regex (from pytorch_pretrained_bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/f3/9260cdb06e36c6b90e5f221d25e43d7756162548d5ccf8b104961b9fe351/regex-2019.04.12.tar.gz (644kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 2.0MB/s \n",
      "\u001b[?25hCollecting botocore<1.13.0,>=1.12.130 (from boto3->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/21/80/6683916e9e1eaf867e09b81a6df6d7a2ba181e3095d2367c7a0f635ed1ed/botocore-1.12.130-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.25,>=1.21.1 (from requests->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" (from botocore<1.13.0,>=1.12.130->boto3->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl\n",
      "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.130->boto3->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.130->boto3->pytorch_pretrained_bert)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: regex\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/eduardo/.cache/pip/wheels/37/42/4c/166c0e4d5e02f8493300ebc7974b508dffdf95c45bdd23e3c1\n",
      "Successfully built regex\n",
      "Installing collected packages: numpy, urllib3, six, python-dateutil, docutils, jmespath, botocore, s3transfer, boto3, idna, certifi, chardet, requests, torch, tqdm, regex, pytorch-pretrained-bert\n",
      "Successfully installed boto3-1.9.130 botocore-1.12.130 certifi-2019.3.9 chardet-3.0.4 docutils-0.14 idna-2.8 jmespath-0.9.4 numpy-1.16.2 python-dateutil-2.8.0 pytorch-pretrained-bert-0.6.1 regex-2019.4.12 requests-2.21.0 s3transfer-0.2.0 six-1.12.0 torch-1.0.1.post2 tqdm-4.31.1 urllib3-1.24.1\n"
     ]
    }
   ],
   "source": [
    "# asegúrate de tener instaladas las siguientes dependencias\n",
    "!pip3 install numpy torchvision_nightly\n",
    "!pip3 install torch_nightly -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
    "!pip3 install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042301/1042301 [00:00<00:00, 1901423.92B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 938668.40B/s]\n",
      "100%|██████████| 548118077/548118077 [00:28<00:00, 19032007.37B/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 68307.35B/s]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F \n",
    "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, definimos una función para:\n",
    "\n",
    "1. tokenizar el texto de entrada y codificarlo como un vector con los pesos obtenidos por el modelo GPT2\n",
    "2. predecir la siguiente palabra más frecuente\n",
    "3. decodificar el vector como una secuencia de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, length=50):\n",
    "    \"\"\"Generate automatic Natural Language from the input text\"\"\"\n",
    "    vec_text = tokenizer.encode(text)\n",
    "    my_input, past = torch.tensor([vec_text]), None\n",
    "    \n",
    "    for _ in range(length):\n",
    "        logits, past = model(my_input, past=past)\n",
    "        my_input = torch.multinomial(F.softmax(logits[:, -1]), 1)\n",
    "        vec_text.append(my_input.item())\n",
    "    \n",
    "    return tokenizer.decode(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After President Trump was pressured by his son-in-law and daughter, Ivanka Trump, to yank 100,000 jobs from the United States from North Korea, many and many full time, new reports emerged that reflect Trump's untruths about the Iron Dome.Internal Cruz campaign counsel Brian Fallon claims only \"90 per cent of generals heard of the principles Trump adopted\" and Earl of Thurmond. He suggests even know people under the\n"
     ]
    }
   ],
   "source": [
    "# defino un texto de entrada\n",
    "text = \"After President Trump was pressured by his son-in-law and daughter\"\n",
    "\n",
    "# y generamos automáticamente las secuencias más probables\n",
    "print(generate(text, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
